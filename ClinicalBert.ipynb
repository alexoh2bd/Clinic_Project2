{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexoh/Downloads/Clinic_Project2/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from xml.etree import ElementTree as ET\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "def parse_clinical_results(root):\n",
    "    \"\"\"\n",
    "    Parse the clinical results section from XML and return structured data.\n",
    "    \"\"\"\n",
    "    clinical_results = {}\n",
    "    \n",
    "    # Find clinical_results section\n",
    "    results_section = root.find('clinical_results')\n",
    "    if results_section is None:\n",
    "        return clinical_results\n",
    "    \n",
    "    # Parse participant flow\n",
    "    participant_flow = {}\n",
    "    flow_section = results_section.find('participant_flow')\n",
    "    if flow_section is not None:\n",
    "        # Parse groups\n",
    "        groups = []\n",
    "        group_list = flow_section.find('group_list')\n",
    "        if group_list is not None:\n",
    "            for group in group_list.findall('group'):\n",
    "                group_data = {\n",
    "                    'group_id': group.get('group_id', ''),\n",
    "                    'title': group.find('title').text if group.find('title') is not None else '',\n",
    "                    'description': group.find('description').text if group.find('description') is not None else ''\n",
    "                }\n",
    "                groups.append(group_data)\n",
    "        \n",
    "        # Parse periods and milestones\n",
    "        periods = []\n",
    "        period_list = flow_section.find('period_list')\n",
    "        if period_list is not None:\n",
    "            for period in period_list.findall('period'):\n",
    "                period_data = {\n",
    "                    'title': period.find('title').text if period.find('title') is not None else '',\n",
    "                    'milestones': []\n",
    "                }\n",
    "                \n",
    "                milestone_list = period.find('milestone_list')\n",
    "                if milestone_list is not None:\n",
    "                    for milestone in milestone_list.findall('milestone'):\n",
    "                        milestone_data = {\n",
    "                            'title': milestone.find('title').text if milestone.find('title') is not None else '',\n",
    "                            'participants': []\n",
    "                        }\n",
    "                        \n",
    "                        participants_list = milestone.find('participants_list')\n",
    "                        if participants_list is not None:\n",
    "                            for participants in participants_list.findall('participants'):\n",
    "                                participant_data = {\n",
    "                                    'group_id': participants.get('group_id', ''),\n",
    "                                    'count': participants.get('count', '')\n",
    "                                }\n",
    "                                milestone_data['participants'].append(participant_data)\n",
    "                        \n",
    "                        period_data['milestones'].append(milestone_data)\n",
    "                \n",
    "                # Parse drop/withdraw reasons\n",
    "                drop_reasons = []\n",
    "                drop_list = period.find('drop_withdraw_reason_list')\n",
    "                if drop_list is not None:\n",
    "                    for reason in drop_list.findall('drop_withdraw_reason'):\n",
    "                        reason_data = {\n",
    "                            'title': reason.find('title').text if reason.find('title') is not None else '',\n",
    "                            'participants': []\n",
    "                        }\n",
    "                        \n",
    "                        participants_list = reason.find('participants_list')\n",
    "                        if participants_list is not None:\n",
    "                            for participants in participants_list.findall('participants'):\n",
    "                                participant_data = {\n",
    "                                    'group_id': participants.get('group_id', ''),\n",
    "                                    'count': participants.get('count', '')\n",
    "                                }\n",
    "                                reason_data['participants'].append(participant_data)\n",
    "                        \n",
    "                        drop_reasons.append(reason_data)\n",
    "                \n",
    "                period_data['drop_withdraw_reasons'] = drop_reasons\n",
    "                periods.append(period_data)\n",
    "        \n",
    "        participant_flow = {\n",
    "            'groups': groups,\n",
    "            'periods': periods\n",
    "        }\n",
    "    \n",
    "    # Parse baseline characteristics\n",
    "    baseline = {}\n",
    "    baseline_section = results_section.find('baseline')\n",
    "    if baseline_section is not None:\n",
    "        # Parse baseline groups\n",
    "        baseline_groups = []\n",
    "        group_list = baseline_section.find('group_list')\n",
    "        if group_list is not None:\n",
    "            for group in group_list.findall('group'):\n",
    "                group_data = {\n",
    "                    'group_id': group.get('group_id', ''),\n",
    "                    'title': group.find('title').text if group.find('title') is not None else '',\n",
    "                    'description': group.find('description').text if group.find('description') is not None else ''\n",
    "                }\n",
    "                baseline_groups.append(group_data)\n",
    "        \n",
    "        # Parse analyzed participants\n",
    "        analyzed_list = []\n",
    "        analyzed_section = baseline_section.find('analyzed_list')\n",
    "        if analyzed_section is not None:\n",
    "            for analyzed in analyzed_section.findall('analyzed'):\n",
    "                analyzed_data = {\n",
    "                    'units': analyzed.find('units').text if analyzed.find('units') is not None else '',\n",
    "                    'scope': analyzed.find('scope').text if analyzed.find('scope') is not None else '',\n",
    "                    'counts': []\n",
    "                }\n",
    "                \n",
    "                count_list = analyzed.find('count_list')\n",
    "                if count_list is not None:\n",
    "                    for count in count_list.findall('count'):\n",
    "                        count_data = {\n",
    "                            'group_id': count.get('group_id', ''),\n",
    "                            'value': count.get('value', '')\n",
    "                        }\n",
    "                        analyzed_data['counts'].append(count_data)\n",
    "                \n",
    "                analyzed_list.append(analyzed_data)\n",
    "        \n",
    "        # Parse baseline measures\n",
    "        measures = []\n",
    "        measure_list = baseline_section.find('measure_list')\n",
    "        if measure_list is not None:\n",
    "            for measure in measure_list.findall('measure'):\n",
    "                measure_data = {\n",
    "                    'title': measure.find('title').text if measure.find('title') is not None else '',\n",
    "                    'description': measure.find('description').text if measure.find('description') is not None else '',\n",
    "                    'units': measure.find('units').text if measure.find('units') is not None else '',\n",
    "                    'param': measure.find('param').text if measure.find('param') is not None else '',\n",
    "                    'classes': []\n",
    "                }\n",
    "                \n",
    "                class_list = measure.find('class_list')\n",
    "                if class_list is not None:\n",
    "                    for class_elem in class_list.findall('class'):\n",
    "                        class_data = {\n",
    "                            'title': class_elem.find('title').text if class_elem.find('title') is not None else '',\n",
    "                            'categories': []\n",
    "                        }\n",
    "                        \n",
    "                        category_list = class_elem.find('category_list')\n",
    "                        if category_list is not None:\n",
    "                            for category in category_list.findall('category'):\n",
    "                                category_data = {\n",
    "                                    'title': category.find('title').text if category.find('title') is not None else '',\n",
    "                                    'measurements': []\n",
    "                                }\n",
    "                                \n",
    "                                measurement_list = category.find('measurement_list')\n",
    "                                if measurement_list is not None:\n",
    "                                    for measurement in measurement_list.findall('measurement'):\n",
    "                                        measurement_data = {\n",
    "                                            'group_id': measurement.get('group_id', ''),\n",
    "                                            'value': measurement.get('value', ''),\n",
    "                                            'spread': measurement.get('spread', ''),\n",
    "                                            'lower_limit': measurement.get('lower_limit', ''),\n",
    "                                            'upper_limit': measurement.get('upper_limit', '')\n",
    "                                        }\n",
    "                                        category_data['measurements'].append(measurement_data)\n",
    "                                \n",
    "                                class_data['categories'].append(category_data)\n",
    "                        \n",
    "                        measure_data['classes'].append(class_data)\n",
    "                \n",
    "                measures.append(measure_data)\n",
    "        \n",
    "        baseline = {\n",
    "            'groups': baseline_groups,\n",
    "            'analyzed_list': analyzed_list,\n",
    "            'measures': measures\n",
    "        }\n",
    "    \n",
    "    # Parse outcomes\n",
    "    outcomes = []\n",
    "    outcome_list = results_section.find('outcome_list')\n",
    "    if outcome_list is not None:\n",
    "        for outcome in outcome_list.findall('outcome'):\n",
    "            outcome_data = {\n",
    "                'type': outcome.find('type').text if outcome.find('type') is not None else '',\n",
    "                'title': outcome.find('title').text if outcome.find('title') is not None else '',\n",
    "                'description': outcome.find('description').text if outcome.find('description') is not None else '',\n",
    "                'time_frame': outcome.find('time_frame').text if outcome.find('time_frame') is not None else '',\n",
    "                'population': outcome.find('population').text if outcome.find('population') is not None else '',\n",
    "                'groups': [],\n",
    "                'measures': []\n",
    "            }\n",
    "            \n",
    "            # Parse outcome groups\n",
    "            group_list = outcome.find('group_list')\n",
    "            if group_list is not None:\n",
    "                for group in group_list.findall('group'):\n",
    "                    group_data = {\n",
    "                        'group_id': group.get('group_id', ''),\n",
    "                        'title': group.find('title').text if group.find('title') is not None else '',\n",
    "                        'description': group.find('description').text if group.find('description') is not None else ''\n",
    "                    }\n",
    "                    outcome_data['groups'].append(group_data)\n",
    "            \n",
    "            # Parse outcome measures\n",
    "            measure_elem = outcome.find('measure')\n",
    "            if measure_elem is not None:\n",
    "                measure_data = {\n",
    "                    'title': measure_elem.find('title').text if measure_elem.find('title') is not None else '',\n",
    "                    'description': measure_elem.find('description').text if measure_elem.find('description') is not None else '',\n",
    "                    'population': measure_elem.find('population').text if measure_elem.find('population') is not None else '',\n",
    "                    'units': measure_elem.find('units').text if measure_elem.find('units') is not None else '',\n",
    "                    'param': measure_elem.find('param').text if measure_elem.find('param') is not None else '',\n",
    "                    'dispersion': measure_elem.find('dispersion').text if measure_elem.find('dispersion') is not None else '',\n",
    "                    'analyzed_list': [],\n",
    "                    'classes': []\n",
    "                }\n",
    "                \n",
    "                # Parse analyzed participants for this measure\n",
    "                analyzed_list = measure_elem.find('analyzed_list')\n",
    "                if analyzed_list is not None:\n",
    "                    for analyzed in analyzed_list.findall('analyzed'):\n",
    "                        analyzed_data = {\n",
    "                            'units': analyzed.find('units').text if analyzed.find('units') is not None else '',\n",
    "                            'scope': analyzed.find('scope').text if analyzed.find('scope') is not None else '',\n",
    "                            'counts': []\n",
    "                        }\n",
    "                        \n",
    "                        count_list = analyzed.find('count_list')\n",
    "                        if count_list is not None:\n",
    "                            for count in count_list.findall('count'):\n",
    "                                count_data = {\n",
    "                                    'group_id': count.get('group_id', ''),\n",
    "                                    'value': count.get('value', '')\n",
    "                                }\n",
    "                                analyzed_data['counts'].append(count_data)\n",
    "                        \n",
    "                        measure_data['analyzed_list'].append(analyzed_data)\n",
    "                \n",
    "                # Parse measure classes\n",
    "                class_list = measure_elem.find('class_list')\n",
    "                if class_list is not None:\n",
    "                    for class_elem in class_list.findall('class'):\n",
    "                        class_data = {\n",
    "                            'title': class_elem.find('title').text if class_elem.find('title') is not None else '',\n",
    "                            'categories': []\n",
    "                        }\n",
    "                        \n",
    "                        category_list = class_elem.find('category_list')\n",
    "                        if category_list is not None:\n",
    "                            for category in category_list.findall('category'):\n",
    "                                category_data = {\n",
    "                                    'title': category.find('title').text if category.find('title') is not None else '',\n",
    "                                    'measurements': []\n",
    "                                }\n",
    "                                \n",
    "                                measurement_list = category.find('measurement_list')\n",
    "                                if measurement_list is not None:\n",
    "                                    for measurement in measurement_list.findall('measurement'):\n",
    "                                        measurement_data = {\n",
    "                                            'group_id': measurement.get('group_id', ''),\n",
    "                                            'value': measurement.get('value', ''),\n",
    "                                            'spread': measurement.get('spread', ''),\n",
    "                                            'lower_limit': measurement.get('lower_limit', ''),\n",
    "                                            'upper_limit': measurement.get('upper_limit', '')\n",
    "                                        }\n",
    "                                        category_data['measurements'].append(measurement_data)\n",
    "                                \n",
    "                                class_data['categories'].append(category_data)\n",
    "                        \n",
    "                        measure_data['classes'].append(class_data)\n",
    "                \n",
    "                outcome_data['measures'].append(measure_data)\n",
    "            \n",
    "            outcomes.append(outcome_data)\n",
    "    \n",
    "    # Parse reported events (adverse events)\n",
    "    reported_events = {}\n",
    "    events_section = results_section.find('reported_events')\n",
    "    if events_section is not None:\n",
    "        reported_events = {\n",
    "            'time_frame': events_section.find('time_frame').text if events_section.find('time_frame') is not None else '',\n",
    "            'desc': events_section.find('desc').text if events_section.find('desc') is not None else '',\n",
    "            'groups': [],\n",
    "            'serious_events': [],\n",
    "            'other_events': []\n",
    "        }\n",
    "        \n",
    "        # Parse event groups\n",
    "        group_list = events_section.find('group_list')\n",
    "        if group_list is not None:\n",
    "            for group in group_list.findall('group'):\n",
    "                group_data = {\n",
    "                    'group_id': group.get('group_id', ''),\n",
    "                    'title': group.find('title').text if group.find('title') is not None else '',\n",
    "                    'description': group.find('description').text if group.find('description') is not None else ''\n",
    "                }\n",
    "                reported_events['groups'].append(group_data)\n",
    "        \n",
    "        # Parse serious events\n",
    "        serious_events = events_section.find('serious_events')\n",
    "        if serious_events is not None:\n",
    "            serious_data = {\n",
    "                'default_vocab': serious_events.get('default_vocab', ''),\n",
    "                'default_assessment': serious_events.get('default_assessment', ''),\n",
    "                'categories': []\n",
    "            }\n",
    "            \n",
    "            category_list = serious_events.find('category_list')\n",
    "            if category_list is not None:\n",
    "                for category in category_list.findall('category'):\n",
    "                    category_data = {\n",
    "                        'title': category.find('title').text if category.find('title') is not None else '',\n",
    "                        'events': []\n",
    "                    }\n",
    "                    \n",
    "                    event_list = category.find('event_list')\n",
    "                    if event_list is not None:\n",
    "                        for event in event_list.findall('event'):\n",
    "                            event_data = {\n",
    "                                'sub_title': event.find('sub_title').text if event.find('sub_title') is not None else '',\n",
    "                                'assessment': event.get('assessment', ''),\n",
    "                                'counts': []\n",
    "                            }\n",
    "                            \n",
    "                            counts_elem = event.find('counts')\n",
    "                            if counts_elem is not None:\n",
    "                                for count_attr in ['subjects_affected', 'subjects_at_risk', 'events']:\n",
    "                                    if counts_elem.get(count_attr):\n",
    "                                        count_data = {\n",
    "                                            'group_id': counts_elem.get('group_id', ''),\n",
    "                                            'type': count_attr,\n",
    "                                            'value': counts_elem.get(count_attr, '')\n",
    "                                        }\n",
    "                                        event_data['counts'].append(count_data)\n",
    "                            \n",
    "                            category_data['events'].append(event_data)\n",
    "                    \n",
    "                    serious_data['categories'].append(category_data)\n",
    "            \n",
    "            reported_events['serious_events'] = serious_data\n",
    "        \n",
    "        # Parse other events (similar structure to serious events)\n",
    "        other_events = events_section.find('other_events')\n",
    "        if other_events is not None:\n",
    "            other_data = {\n",
    "                'frequency_threshold': other_events.get('frequency_threshold', ''),\n",
    "                'default_vocab': other_events.get('default_vocab', ''),\n",
    "                'default_assessment': other_events.get('default_assessment', ''),\n",
    "                'categories': []\n",
    "            }\n",
    "            \n",
    "            category_list = other_events.find('category_list')\n",
    "            if category_list is not None:\n",
    "                for category in category_list.findall('category'):\n",
    "                    category_data = {\n",
    "                        'title': category.find('title').text if category.find('title') is not None else '',\n",
    "                        'events': []\n",
    "                    }\n",
    "                    \n",
    "                    event_list = category.find('event_list')\n",
    "                    if event_list is not None:\n",
    "                        for event in event_list.findall('event'):\n",
    "                            event_data = {\n",
    "                                'sub_title': event.find('sub_title').text if event.find('sub_title') is not None else '',\n",
    "                                'assessment': event.get('assessment', ''),\n",
    "                                'counts': []\n",
    "                            }\n",
    "                            \n",
    "                            counts_elem = event.find('counts')\n",
    "                            if counts_elem is not None:\n",
    "                                for count_attr in ['subjects_affected', 'subjects_at_risk', 'events']:\n",
    "                                    if counts_elem.get(count_attr):\n",
    "                                        count_data = {\n",
    "                                            'group_id': counts_elem.get('group_id', ''),\n",
    "                                            'type': count_attr,\n",
    "                                            'value': counts_elem.get(count_attr, '')\n",
    "                                        }\n",
    "                                        event_data['counts'].append(count_data)\n",
    "                            \n",
    "                            category_data['events'].append(event_data)\n",
    "                    \n",
    "                    other_data['categories'].append(category_data)\n",
    "            \n",
    "            reported_events['other_events'] = other_data\n",
    "    \n",
    "    # Parse agreements and point of contact\n",
    "    agreements = {}\n",
    "    agreements_section = results_section.find('certain_agreements')\n",
    "    if agreements_section is not None:\n",
    "        agreements = {\n",
    "            'pi_employee': agreements_section.find('pi_employee').text if agreements_section.find('pi_employee') is not None else '',\n",
    "            'restrictive_agreement': agreements_section.find('restrictive_agreement').text if agreements_section.find('restrictive_agreement') is not None else ''\n",
    "        }\n",
    "    \n",
    "    point_of_contact = {}\n",
    "    contact_section = results_section.find('point_of_contact')\n",
    "    if contact_section is not None:\n",
    "        point_of_contact = {\n",
    "            'name_or_title': contact_section.find('name_or_title').text if contact_section.find('name_or_title') is not None else '',\n",
    "            'organization': contact_section.find('organization').text if contact_section.find('organization') is not None else '',\n",
    "            'phone': contact_section.find('phone').text if contact_section.find('phone') is not None else '',\n",
    "            'email': contact_section.find('email').text if contact_section.find('email') is not None else ''\n",
    "        }\n",
    "    \n",
    "    # Assemble clinical results\n",
    "    clinical_results = {\n",
    "        'participant_flow': participant_flow,\n",
    "        'baseline': baseline,\n",
    "        'outcomes': outcomes,\n",
    "        'reported_events': reported_events,\n",
    "        'certain_agreements': agreements,\n",
    "        'point_of_contact': point_of_contact\n",
    "    }\n",
    "    \n",
    "    return clinical_results\n",
    "\n",
    "\n",
    "def xmlfile2results(xml_file):\n",
    "    \"\"\"\n",
    "    Parse clinical trial XML file and return a dictionary with extracted data.\n",
    "    \"\"\"\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # Basic study identifiers\n",
    "    nctid = root.find('id_info/nct_id').text if root.find('id_info/nct_id') is not None else ''\n",
    "    org_study_id = root.find('id_info/org_study_id').text if root.find('id_info/org_study_id') is not None else ''\n",
    "    url = root.find('required_header/url').text if root.find('required_header/url') is not None else ''\n",
    "    \n",
    "    # Titles - handle both brief_title and official_title\n",
    "    brief_title = root.find('brief_title').text if root.find('brief_title') is not None else ''\n",
    "    official_title = root.find('official_title').text if root.find('official_title') is not None else ''\n",
    "    \n",
    "    # Sponsors and collaborators\n",
    "    lead_sponsor = ''\n",
    "    collaborators = []\n",
    "    sponsors = root.find('sponsors')\n",
    "    if sponsors is not None:\n",
    "        lead_sponsor_elem = sponsors.find('lead_sponsor/agency')\n",
    "        if lead_sponsor_elem is not None:\n",
    "            lead_sponsor = lead_sponsor_elem.text\n",
    "        \n",
    "        for collab in sponsors.findall('collaborator/agency'):\n",
    "            if collab is not None:\n",
    "                collaborators.append(collab.text)\n",
    "    \n",
    "    # Study descriptions\n",
    "    brief_summary = ''\n",
    "    brief_summary_elem = root.find('brief_summary/textblock')\n",
    "    if brief_summary_elem is not None:\n",
    "        brief_summary = brief_summary_elem.text.strip() if brief_summary_elem.text else ''\n",
    "    \n",
    "    detailed_description = ''\n",
    "    detailed_description_elem = root.find('detailed_description/textblock')\n",
    "    if detailed_description_elem is not None:\n",
    "        detailed_description = detailed_description_elem.text.strip() if detailed_description_elem.text else ''\n",
    "    \n",
    "    # Study type and phase\n",
    "    study_type = root.find('study_type').text if root.find('study_type') is not None else ''\n",
    "    phase = root.find('phase').text if root.find('phase') is not None else ''\n",
    "    \n",
    "    # Status and dates\n",
    "    overall_status = root.find('overall_status').text if root.find('overall_status') is not None else ''\n",
    "    why_stopped = root.find('why_stopped').text if root.find('why_stopped') is not None else ''\n",
    "\n",
    "    # create an inferred label from the overall_status and why_stopped\n",
    "    \n",
    "    \n",
    "    # Handle dates with potential type attributes\n",
    "    start_date = ''\n",
    "    start_date_type = ''\n",
    "    start_date_elem = root.find('start_date')\n",
    "    if start_date_elem is not None:\n",
    "        start_date = start_date_elem.text if start_date_elem.text else ''\n",
    "        start_date_type = start_date_elem.get('type', '')\n",
    "    \n",
    "    completion_date = ''\n",
    "    completion_date_type = ''\n",
    "    # Check both completion_date and primary_completion_date\n",
    "    completion_date_elem = root.find('completion_date')\n",
    "    if completion_date_elem is None:\n",
    "        completion_date_elem = root.find('primary_completion_date')\n",
    "    if completion_date_elem is not None:\n",
    "        completion_date = completion_date_elem.text if completion_date_elem.text else ''\n",
    "        completion_date_type = completion_date_elem.get('type', '')\n",
    "    \n",
    "    study_first_posted = ''\n",
    "    study_first_posted_elem = root.find('study_first_posted')\n",
    "    if study_first_posted_elem is not None:\n",
    "        study_first_posted = study_first_posted_elem.text if study_first_posted_elem.text else ''\n",
    "    \n",
    "    # Interventions - handle all types, not just drugs\n",
    "    interventions = []\n",
    "    for intervention in root.findall('intervention'):\n",
    "        intervention_data = {}\n",
    "        intervention_type_elem = intervention.find('intervention_type')\n",
    "        intervention_name_elem = intervention.find('intervention_name')\n",
    "        intervention_desc_elem = intervention.find('description')\n",
    "        \n",
    "        if intervention_type_elem is not None:\n",
    "            intervention_data['type'] = intervention_type_elem.text\n",
    "        if intervention_name_elem is not None:\n",
    "            intervention_data['name'] = intervention_name_elem.text\n",
    "        if intervention_desc_elem is not None:\n",
    "            intervention_data['description'] = intervention_desc_elem.text\n",
    "\n",
    "    # add intervention Mesh terms\n",
    "    # Get only intervention mesh terms\n",
    "    intervention_section = root.find('intervention_browse')\n",
    "    if intervention_section is not None:\n",
    "        intervention_mesh_terms = [term.text.strip() for term in intervention_section.findall('mesh_term')]\n",
    "    else:\n",
    "        intervention_mesh_terms = []\n",
    "    \n",
    "    # Extract drug interventions separately for backward compatibility\n",
    "    drug_interventions = [i['name'] for i in interventions if i.get('type') == 'Drug' and 'name' in i]\n",
    "    \n",
    "    # Arm groups\n",
    "    arm_groups = []\n",
    "    for ag in root.findall('arm_group'):\n",
    "        arm_data = {}\n",
    "        label_elem = ag.find('arm_group_label')\n",
    "        desc_elem = ag.find('description')\n",
    "        type_elem = ag.find('arm_group_type')\n",
    "        \n",
    "        if label_elem is not None:\n",
    "            arm_data['label'] = label_elem.text\n",
    "        if desc_elem is not None:\n",
    "            arm_data['description'] = desc_elem.text\n",
    "        if type_elem is not None:\n",
    "            arm_data['type'] = type_elem.text\n",
    "        \n",
    "        if arm_data:\n",
    "            arm_groups.append(arm_data)\n",
    "    \n",
    "    # Study design information\n",
    "    study_design_info = {}\n",
    "    sdi = root.find('study_design_info')\n",
    "    if sdi is not None:\n",
    "        for child in sdi:\n",
    "            if child.text:\n",
    "                study_design_info[child.tag] = child.text\n",
    "\n",
    "    # Clinical Results\n",
    "    \n",
    "    # Primary outcome\n",
    "    primary_outcomes = []\n",
    "    for po in root.findall('primary_outcome'):\n",
    "        outcome_data = {}\n",
    "        measure_elem = po.find('measure')\n",
    "        time_frame_elem = po.find('time_frame')\n",
    "        description_elem = po.find('description')\n",
    "        \n",
    "        if measure_elem is not None:\n",
    "            outcome_data['measure'] = measure_elem.text\n",
    "        if time_frame_elem is not None:\n",
    "            outcome_data['time_frame'] = time_frame_elem.text\n",
    "        if description_elem is not None:\n",
    "            outcome_data['description'] = description_elem.text\n",
    "        \n",
    "        if outcome_data:\n",
    "            primary_outcomes.append(outcome_data)\n",
    "    \n",
    "    # Secondary outcomes\n",
    "    secondary_outcomes = []\n",
    "    for so in root.findall('secondary_outcome'):\n",
    "        outcome_data = {}\n",
    "        measure_elem = so.find('measure')\n",
    "        time_frame_elem = so.find('time_frame')\n",
    "        description_elem = so.find('description')\n",
    "        \n",
    "        if measure_elem is not None:\n",
    "            outcome_data['measure'] = measure_elem.text\n",
    "        if time_frame_elem is not None:\n",
    "            outcome_data['time_frame'] = time_frame_elem.text\n",
    "        if description_elem is not None:\n",
    "            outcome_data['description'] = description_elem.text\n",
    "        \n",
    "        if outcome_data:\n",
    "            secondary_outcomes.append(outcome_data)\n",
    "    \n",
    "    # Conditions/indications\n",
    "    conditions = [condition.text.strip() for condition in root.findall('condition')]\n",
    "    \n",
    "    # MeSH terms for conditions\n",
    "    conditions_mesh_terms = []\n",
    "    conditions_section = root.find('condition_browse')\n",
    "    if conditions_section is not None:\n",
    "        conditions_mesh_terms = [term.text.strip() for term in conditions_section.findall('mesh_term')]\n",
    "    \n",
    "    # Enrollment\n",
    "    enrollment = ''\n",
    "    enrollment_type = ''\n",
    "    enrollment_elem = root.find('enrollment')\n",
    "    if enrollment_elem is not None:\n",
    "        enrollment = enrollment_elem.text if enrollment_elem.text else ''\n",
    "        enrollment_type = enrollment_elem.get('type', '')\n",
    "    \n",
    "    # Eligibility criteria\n",
    "    criteria = ''\n",
    "    criteria_elem = root.find('eligibility/criteria/textblock')\n",
    "    if criteria_elem is not None:\n",
    "        criteria = criteria_elem.text.strip() if criteria_elem.text else ''\n",
    "    \n",
    "    # Gender, age constraints\n",
    "    gender = ''\n",
    "    minimum_age = ''\n",
    "    maximum_age = ''\n",
    "    healthy_volunteers = ''\n",
    "    \n",
    "    eligibility = root.find('eligibility')\n",
    "    if eligibility is not None:\n",
    "        gender_elem = eligibility.find('gender')\n",
    "        min_age_elem = eligibility.find('minimum_age')\n",
    "        max_age_elem = eligibility.find('maximum_age')\n",
    "        healthy_elem = eligibility.find('healthy_volunteers')\n",
    "        \n",
    "        if gender_elem is not None:\n",
    "            gender = gender_elem.text\n",
    "        if min_age_elem is not None:\n",
    "            minimum_age = min_age_elem.text\n",
    "        if max_age_elem is not None:\n",
    "            maximum_age = max_age_elem.text\n",
    "        if healthy_elem is not None:\n",
    "            healthy_volunteers = healthy_elem.text\n",
    "    \n",
    "    # Number of groups\n",
    "    number_of_groups = root.find('number_of_groups').text if root.find('number_of_groups') is not None else ''\n",
    "    \n",
    "    # Locations\n",
    "    locations = []\n",
    "    for loc in root.findall('location'):\n",
    "        location_data = {}\n",
    "        facility = loc.find('facility')\n",
    "        if facility is not None:\n",
    "            name_elem = facility.find('name')\n",
    "            if name_elem is not None:\n",
    "                location_data['facility_name'] = name_elem.text\n",
    "            \n",
    "            address = facility.find('address')\n",
    "            if address is not None:\n",
    "                city_elem = address.find('city')\n",
    "                state_elem = address.find('state')\n",
    "                zip_elem = address.find('zip')\n",
    "                country_elem = address.find('country')\n",
    "                \n",
    "                if city_elem is not None:\n",
    "                    location_data['city'] = city_elem.text\n",
    "                if state_elem is not None:\n",
    "                    location_data['state'] = state_elem.text\n",
    "                if zip_elem is not None:\n",
    "                    location_data['zip'] = zip_elem.text\n",
    "                if country_elem is not None:\n",
    "                    location_data['country'] = country_elem.text\n",
    "        \n",
    "        # Location status\n",
    "        status_elem = loc.find('status')\n",
    "        if status_elem is not None:\n",
    "            location_data['status'] = status_elem.text\n",
    "        \n",
    "        # Contact information\n",
    "        contact = loc.find('contact')\n",
    "        if contact is not None:\n",
    "            contact_name = contact.find('last_name')\n",
    "            contact_email = contact.find('email')\n",
    "            contact_phone = contact.find('phone')\n",
    "            \n",
    "            if contact_name is not None:\n",
    "                location_data['contact_name'] = contact_name.text\n",
    "            if contact_email is not None:\n",
    "                location_data['contact_email'] = contact_email.text\n",
    "            if contact_phone is not None:\n",
    "                location_data['contact_phone'] = contact_phone.text\n",
    "        \n",
    "        if location_data:\n",
    "            locations.append(location_data)\n",
    "    \n",
    "    # Overall contacts\n",
    "    overall_contact = {}\n",
    "    contact = root.find('overall_contact')\n",
    "    if contact is not None:\n",
    "        name_elem = contact.find('last_name')\n",
    "        email_elem = contact.find('email')\n",
    "        phone_elem = contact.find('phone')\n",
    "        \n",
    "        if name_elem is not None:\n",
    "            overall_contact['name'] = name_elem.text\n",
    "        if email_elem is not None:\n",
    "            overall_contact['email'] = email_elem.text\n",
    "        if phone_elem is not None:\n",
    "            overall_contact['phone'] = phone_elem.text\n",
    "    \n",
    "    # Oversight info\n",
    "    oversight_info = {}\n",
    "    oversight = root.find('oversight_info')\n",
    "    if oversight is not None:\n",
    "        for child in oversight:\n",
    "            if child.text:\n",
    "                oversight_info[child.tag] = child.text\n",
    "    \n",
    "    # Keywords\n",
    "    keywords = []\n",
    "    for keyword in root.findall('keyword'):\n",
    "        if keyword.text:\n",
    "            keywords.append(keyword.text)\n",
    "\n",
    "    # Clinical Results\n",
    "    clinical_results = parse_clinical_results(root)\n",
    "    \n",
    "    # Assemble the complete data dictionary\n",
    "    data = {\n",
    "        'nctid': nctid,\n",
    "        'org_study_id': org_study_id,\n",
    "        'url': url,\n",
    "        'brief_title': brief_title,\n",
    "        'official_title': official_title,\n",
    "        'lead_sponsor': lead_sponsor,\n",
    "        'collaborators': collaborators,\n",
    "        'brief_summary': brief_summary,\n",
    "        'detailed_description': detailed_description,\n",
    "        'study_type': study_type,\n",
    "        'phase': phase,\n",
    "        'overall_status': overall_status,\n",
    "        'why_stopped': why_stopped,\n",
    "        'start_date': start_date,\n",
    "        'start_date_type': start_date_type,\n",
    "        'completion_date': completion_date,\n",
    "        'completion_date_type': completion_date_type,\n",
    "        'study_first_posted': study_first_posted,\n",
    "        'interventions': interventions,\n",
    "        'intervention_mesh_terms': intervention_mesh_terms,\n",
    "        'drug_interventions': drug_interventions,  # for backward compatibility\n",
    "        'arm_groups': arm_groups,\n",
    "        'study_design_info': study_design_info,\n",
    "        'primary_outcomes': primary_outcomes,\n",
    "        'secondary_outcomes': secondary_outcomes,\n",
    "        'conditions': conditions,\n",
    "        'conditions_mesh_terms': conditions_mesh_terms,\n",
    "        'enrollment': enrollment,\n",
    "        'enrollment_type': enrollment_type,\n",
    "        'criteria': criteria,\n",
    "        'gender': gender,\n",
    "        'minimum_age': minimum_age,\n",
    "        'maximum_age': maximum_age,\n",
    "        'healthy_volunteers': healthy_volunteers,\n",
    "        'number_of_groups': number_of_groups,\n",
    "        'locations': locations,\n",
    "        'overall_contact': overall_contact,\n",
    "        'oversight_info': oversight_info,\n",
    "        'keywords': keywords,\n",
    "        'clinical_results': clinical_results\n",
    "    }\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "chunk_size = 100\n",
    "feat_cats = ['brief_summary', 'detailed_description']\n",
    "pdir = \"data_processed/\"\n",
    "os.makedirs(pdir, exist_ok=True)\n",
    "\n",
    "# Lists for final labels\n",
    "overall_statuses = []\n",
    "clinical_results = []\n",
    "phases = []\n",
    "\n",
    "# Temporary list for text features\n",
    "X = []\n",
    "\n",
    "# Loop through all folders\n",
    "folders = os.listdir('raw_data/')[:200]\n",
    "for idx, folder in enumerate(folders):\n",
    "    base = os.path.join('raw_data', folder)\n",
    "    if not os.path.isdir(base):\n",
    "        continue\n",
    "    \n",
    "    for file in os.listdir(base):\n",
    "        xml_path = os.path.join(base, file)\n",
    "        try:\n",
    "            data = xmlfile2results(xml_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {xml_path}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Skip entries with missing phase\n",
    "        phase = data.get('phase', '').strip()\n",
    "        if not phase or phase.upper() == 'N/A':\n",
    "            continue\n",
    "        \n",
    "        # Collect text features safely\n",
    "        textfeats = \"\"\n",
    "        for feat in feat_cats:\n",
    "            textfeats += data.get(feat, '')\n",
    "        textfeats += \"\".join(data.get('interventions', []))\n",
    "        for po in data.get('primary_outcomes', []):\n",
    "            textfeats += po.get('measure', '')\n",
    "            textfeats += \"\".join(po.get('description', []))\n",
    "        \n",
    "        X.append(textfeats)\n",
    "        overall_statuses.append(data.get('overall_status'))\n",
    "        clinical_results.append(data.get('clinical_results'))\n",
    "        phases.append(phase)\n",
    "    \n",
    "    # Save chunk if reached chunk size\n",
    "    # if (idx + 1) % chunk_size == 0:\n",
    "    #     np.save(os.path.join(pdir, f\"X_{idx+1}.npy\"), np.array(X_chunk,dtype=object), allow_pickle=True)\n",
    "    #     X_chunk.clear()\n",
    "\n",
    "# Save any remaining data\n",
    "# if (idx + 1) % chunk_size == 0:\n",
    "#     arr = np.asarray(X_chunk, dtype=object)   # <-- FORCE object array\n",
    "#     np.save(os.path.join(pdir, f\"X_{idx+1}.npy\"), arr, allow_pickle=True)\n",
    "#     X_chunk.clear()\n",
    "\n",
    "# # Save any remaining data\n",
    "# if X_chunk:\n",
    "#     arr = np.asarray(X_chunk, dtype=object)   # <-- FORCE object array\n",
    "#     np.save(os.path.join(pdir, \"X_final.npy\"), arr, allow_pickle=True)\n",
    "#     X_chunk.clear()\n",
    "\n",
    "\n",
    "# Create binary target y\n",
    "y = np.array([\n",
    "    1 if (str(overall_statuses[i]).lower() == 'completed' \n",
    "          and clinical_results[i] is not None\n",
    "          and phases[i].lower() in ['phase 2/phase 3','phase 3','phase 4'])\n",
    "    else 0\n",
    "    for i in range(len(overall_statuses))\n",
    "])\n",
    "\n",
    "# Save labels\n",
    "# np.save(os.path.join(pdir, \"y.npy\"), y)\n",
    "# np.save(os.path.join(pdir, \"phases.npy\"), np.array(phases), allow_pickle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=random_seed, shuffle=True, test_size=0.1\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, stratify=y_train_full, random_state=random_seed, shuffle=True, test_size=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clinical Bert<br>\n",
    "[HuggingFace](https://huggingface.co/medicalai/ClinicalBERT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch import nn\n",
    "import torch\n",
    "from tqdm import  tqdm\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
    "        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'label': torch.tensor(label)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ClinicalBERT\n",
    "Inspiration from this https://medium.com/@khang.pham.exxact/text-classification-with-bert-7afaacc5e49b\n",
    "'''\n",
    "\n",
    "class ClinicalBERTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.device= 'mps' if torch.mps.is_available() else 'cpu'\n",
    "        self.num_classes=2\n",
    "        self.bert_model_name = \"medicalai/ClinicalBERT\"\n",
    "        super(ClinicalBERTClassifier, self).__init__()\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.bert_model_name)\n",
    "        self.model = AutoModel.from_pretrained(\"medicalai/ClinicalBERT\")\n",
    "        # Output space -> Dropout -> binary classes\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.model.config.hidden_size, self.num_classes)\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_emb = outputs.last_hidden_state[:,0,:]\n",
    "        x = self.dropout(cls_emb)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "    \n",
    "def train(model, data_loader, optimizer, device, loss_fn):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(data_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "    \n",
    "        \n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "            preds_list.extend(preds.cpu().tolist())\n",
    "            labels_list.extend(labels.cpu().tolist())\n",
    "\n",
    "    return accuracy_score(labels_list, preds_list), classification_report(labels_list, preds_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ClinicalBERTClassifier()\n",
    "batch_size= 16\n",
    "max_len = 256\n",
    "trainset = TextDataset(X_train, y_train, clf.tokenizer, max_len)\n",
    "valset = TextDataset(X_val, y_val, clf.tokenizer, max_len)\n",
    "testset = TextDataset(X_test, y_test, clf.tokenizer, max_len)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size= batch_size)\n",
    "testloader = DataLoader(testset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "num_epochs = 10\n",
    "\n",
    "learning_rate = 2e-5\n",
    "optimizer = torch.optim.AdamW(clf.parameters(), lr=learning_rate)\n",
    "total_steps = len(trainloader) * num_epochs\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 14/2939 [00:17<59:57,  1.23s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     accuracy, report = evaluate(clf, valloader, clf.device)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mValidation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, data_loader, optimizer, scheduler, device)\u001b[39m\n\u001b[32m     38\u001b[39m loss = nn.CrossEntropyLoss()(outputs, labels)\n\u001b[32m     39\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m scheduler.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Clinic_Project2/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:133\u001b[39m, in \u001b[36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    131\u001b[39m opt = opt_ref()\n\u001b[32m    132\u001b[39m opt._opt_called = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Clinic_Project2/venv/lib/python3.12/site-packages/torch/optim/optimizer.py:517\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    512\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    513\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    514\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    520\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Clinic_Project2/venv/lib/python3.12/site-packages/torch/optim/optimizer.py:82\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     84\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Clinic_Project2/venv/lib/python3.12/site-packages/torch/optim/adam.py:247\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    235\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    237\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    238\u001b[39m         group,\n\u001b[32m    239\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m         state_steps,\n\u001b[32m    245\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Clinic_Project2/venv/lib/python3.12/site-packages/torch/optim/optimizer.py:150\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Clinic_Project2/venv/lib/python3.12/site-packages/torch/optim/adam.py:953\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    951\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Clinic_Project2/venv/lib/python3.12/site-packages/torch/optim/adam.py:535\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    533\u001b[39m         denom = (max_exp_avg_sqs[i].sqrt() / bias_correction2_sqrt).add_(eps)\n\u001b[32m    534\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m         denom = \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    537\u001b[39m     param.addcdiv_(exp_avg, denom, value=-step_size)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    539\u001b[39m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    train(clf, trainloader, optimizer, scheduler, clf.device )\n",
    "    accuracy, report = evaluate(clf, valloader, clf.device)\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
